{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "258S0HCHQC3o",
        "outputId": "59719363-ea50-4907-8485-979b3e6d2df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        }
      },
      "source": [
        "pip install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 39kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 48.6MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.35.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (50.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=f0c284329acccd72ec012fff6c2b4f626f7da4611e4a948c5af35f00a282d24e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, gast, keras-applications, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LgzsKEcO9pT",
        "outputId": "a0c8aced-09c3-4ec1-f5d7-f373f7f0104d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "d = 1\n",
        "if d == 1 :\n",
        "  !git clone https://github.com/Skuldur/Classical-Piano-Composer\n",
        "  files_path ='/content/Classical-Piano-Composer/midi_songs'\n",
        "elif d == 2 :\n",
        "  !wget https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip\n",
        "  !unzip maestro-v2.0.0-midi.zip\n",
        "  files_path = '/content/maestro-v2.0.0/2018'\n",
        "else:\n",
        "  !git clone https://github.com/corynguyen19/midi-lstm-gan/\n",
        "  files_path = ''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Classical-Piano-Composer'...\n",
            "remote: Enumerating objects: 334, done.\u001b[K\n",
            "remote: Total 334 (delta 0), reused 0 (delta 0), pack-reused 334\u001b[K\n",
            "Receiving objects: 100% (334/334), 721.79 MiB | 17.26 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPZ_wnmMcj0c",
        "outputId": "1aa94b0a-b124-418a-d3ee-1b5559d06726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import glob\n",
        "from tensorflow.compat.v1 import keras\n",
        "\n",
        "from __future__ import print_function, division\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "from tensorflow.compat.v1.keras.layers import Input, Dense, Reshape, Dropout, CuDNNLSTM, Bidirectional\n",
        "from tensorflow.compat.v1.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from tensorflow.compat.v1.keras.layers import LeakyReLU\n",
        "from tensorflow.compat.v1.keras.models import Sequential, Model\n",
        "from tensorflow.compat.v1.keras.optimizers import Adam\n",
        "from tensorflow.compat.v1.keras.utils import to_categorical,plot_model\n",
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "\n",
        "def get_notes():\n",
        "    \"\"\" Get all the notes and chords from the midi files \"\"\"\n",
        "    notes = []\n",
        "\n",
        "    for file in glob.glob(files_path+\"/*.mid\"):\n",
        "        midi = converter.parse(file)\n",
        "\n",
        "        print(\"Parsing %s\" % file)\n",
        "        notes_to_parse = None\n",
        "\n",
        "        try: # file has instrument parts\n",
        "            s2 = instrument.partitionByInstrument(midi)\n",
        "            notes_to_parse = s2.parts[0].recurse() \n",
        "        except: # file has notes in a flat structure\n",
        "            notes_to_parse = midi.flat.notes\n",
        "            \n",
        "        for element in notes_to_parse:\n",
        "            if isinstance(element, note.Note):\n",
        "                notes.append(str(element.pitch))\n",
        "            elif isinstance(element, chord.Chord):\n",
        "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "            elif isinstance(element, note.Rest):\n",
        "                notes.append('SC')    \n",
        "    with open('GAN_NoteList','wb') as F:\n",
        "      pickle.dump(notes,F)       \n",
        "    \n",
        "    return notes\n",
        "\n",
        "def prepare_sequences(notes):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    sequence_length = 100\n",
        "    n_vocab=len(set(notes))\n",
        "\n",
        "    # Get all possible notes (pitch, chords, SC)\n",
        "    noteNames = sorted(set(item for item in notes))\n",
        "\n",
        "    # Create a dictionary to map pitches to integers\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(noteNames))\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length] #the next note in the seq\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input) #how many seq to we have \n",
        "\n",
        "    # Reshape the input into a format compatible with LSTM layers\n",
        "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    \n",
        "    # Normalize input between -1 and 1\n",
        "    network_input = (network_input - float(n_vocab)/2) / (float(n_vocab)/2)\n",
        "    network_output = to_categorical(network_output)\n",
        "\n",
        "    return (network_input, network_output)\n",
        "\n",
        "def generate_notes(model, network_input):\n",
        "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "        \n",
        "    n_vocab=len(set(notes))\n",
        "    \n",
        "\n",
        "    # pick a random sequence from the input as a starting point for the prediction\n",
        "    start = numpy.random.randint(0, len(network_input)-1)\n",
        "    \n",
        "    # Get pitch names and store in a dictionary\n",
        "    noteNames = sorted(set(item for item in notes))\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(noteNames))\n",
        "\n",
        "    pattern = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate 500 notes\n",
        "    for note_index in range(500):\n",
        "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        #prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "        index = numpy.argmax(prediction)\n",
        "        #ind = np.unravel_index(np.argmax(a, axis=None), a.shape) result=predection[ind] !\n",
        "        result = int_to_note[index]\n",
        "        prediction_output.append(result)\n",
        "        \n",
        "        pattern = numpy.append(pattern,index)\n",
        "        #pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)] #len(pattern)-1 ?\n",
        "\n",
        "    return prediction_output\n",
        "\n",
        "def create_midi(prediction_output, filename):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for item in prediction_output:\n",
        "        pattern = item[0]\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    midi_stream.write('midi', fp='{}.mid'.format(filename))\n",
        "\n",
        "class GAN():\n",
        "    def __init__(self, rows):\n",
        "        self.seq_length = rows\n",
        "        self.seq_shape = (self.seq_length, 1)\n",
        "        self.latent_dim = 1000\n",
        "        self.disc_loss = []\n",
        "        self.gen_loss =[]\n",
        "        self.notes = get_notes()\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates note sequences\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        generated_seq = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        validity = self.discriminator(generated_seq)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(CuDNNLSTM(512, input_shape=self.seq_shape, return_sequences=True)) #faster then LSTM \n",
        "        model.add(Bidirectional(CuDNNLSTM(512)))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid')) #binary_classification\n",
        "        model.summary()\n",
        "        plot_model(model,to_file='descriminator.png',show_shapes=True)\n",
        "        seq = Input(shape=self.seq_shape)\n",
        "        validity = model(seq)\n",
        "\n",
        "        return Model(seq, validity)\n",
        "      \n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.seq_shape))\n",
        "        model.summary()\n",
        "        plot_model(model,to_file='generator.png',show_shapes=True)\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        seq = model(noise)\n",
        "\n",
        "        return Model(noise, seq)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "\n",
        "        X_train, y_train = prepare_sequences(self.notes)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        real = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "        \n",
        "        # Training the model\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # Training the discriminator\n",
        "            # Select a random batch of note sequences\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            real_seqs = X_train[idx]\n",
        "\n",
        "            #noise = np.random.choice(range(484), (batch_size, self.latent_dim))\n",
        "            #noise = (noise-242)/242\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # Generate a batch of new note sequences\n",
        "            gen_seqs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) #la moyenne of the lost fuction of the discriminator\n",
        "\n",
        "\n",
        "            # Train the generator (to have the discriminator label samples as real)\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            g_loss = self.combined.train_on_batch(noise, real) \n",
        "\n",
        "            # Print the progress and save into loss lists\n",
        "            if epoch % sample_interval == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "              self.disc_loss.append(d_loss[0])\n",
        "              self.gen_loss.append(g_loss)\n",
        "        self.generator.save('GAN_model.h5')  \n",
        "\n",
        "        \n",
        "        \n",
        "    def generate(self):\n",
        "        # Get notes names and store in a dictionary\n",
        "        generated_tons = []\n",
        "        base_tons = { 'C':0, 'C#':1, 'D':2, 'D#':3, 'E':4, 'F':5, 'F#':6, 'G':7, 'G#':8, 'A':9, 'A#':10, 'B':11}\n",
        "   \n",
        "        noteNames = sorted(set(item for item in self.notes))\n",
        "        int_to_note = dict((number, note) for number, note in enumerate(noteNames))\n",
        "        \n",
        "        # Use random noise to generate sequences\n",
        "        noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
        "        predictions = self.generator.predict(noise)\n",
        "        \n",
        "        pred_notes = [x*242+242 for x in predictions[0]]\n",
        "        print(int_to_note)\n",
        "        print(pred_notes)\n",
        "        pred_notes = [int_to_note.get(int(x),\"0\") for x in pred_notes]\n",
        "\n",
        "        generated_tons = [x for x in pred_notes if (x[0].isalpha() and x!='SC')]\n",
        "        print(generated_tons)\n",
        "        print(\"tone span =\"+str(base_tons[max(generated_tons)[0]]+8*int(max(generated_tons)[-1])-base_tons[min(generated_tons)[0]]+8*int(min(generated_tons)[-1])))\n",
        "        print(\"unique tones = \"+str(len(set(pred_notes))))\n",
        "        print(pred_notes)\n",
        "        create_midi(pred_notes, 'GAN_')\n",
        "    def plot_loss(self):\n",
        "        plt.plot(self.disc_loss, c='red')\n",
        "        plt.plot(self.gen_loss, c='blue')\n",
        "        plt.title(\"GAN Loss per Epoch\")\n",
        "        plt.legend(['Discriminator', 'Generator'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.savefig('GAN_Loss_per_Epoch_final.png', transparent=True)\n",
        "        plt.close()\n",
        "if __name__ == '__main__':\n",
        "  gan = GAN(rows=100)    \n",
        "  gan.train(epochs=600, batch_size=32, sample_interval=1)\n",
        "  gan.plot_loss()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing /content/Classical-Piano-Composer/midi_songs/ahead_on_our_way_piano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/gerudo.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/8.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/DOS.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ff4-airship.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ff4pclov.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Ff4-BattleLust.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Oppressed.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Finalfantasy6fanfarecomplete.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/FF4.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/FFIXQuMarshP.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Suteki_Da_Ne_(Piano_Version).mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Eternal_Harvest.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/tifap.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ViviinAlexandria.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Ff7-Cinco.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/sandy.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/decisive.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Life_Stream.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/FF3_Battle_(Piano).mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/0fithos.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/traitor.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/AT.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/great_war.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/cosmo.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/FFIX_Piano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Cids.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/thenightmarebegins.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Zelda_Overworld.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ff1battp.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/roseofmay-piano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/z_aeristhemepiano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/HighwindTakestotheSkies.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/path_of_repentance.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ff7themep.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/VincentPiano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/dontbeafraid.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/mining.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Fierce_Battle_(Piano).mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/BlueStone_LastDungeon.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/lurk_in_dark.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/sera_.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ff8-lfp.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/braska.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Final_Fantasy_7_-_Judgement_Day_Piano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/thoughts.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Final_Fantasy_Matouyas_Cave_Piano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/fortresscondor.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/EyesOnMePiano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/FFIII_Edgar_And_Sabin_Piano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ultros.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/tpirtsd-piano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/OTD5YA.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/bcm.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/redwings.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/pkelite4.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/sobf.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ff6shap.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/rufus.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/JENOVA.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/electric_de_chocobo.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/balamb.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Fyw_piano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/figaro.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ultimafro.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/FF8_Shuffle_or_boogie_pc.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/costadsol.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Finalfantasy5gilgameshp.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/waltz_de_choco.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Kingdom_Hearts_Traverse_Town.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ff11_awakening_piano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ff4_piano_collections-main_theme.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Fiend_Battle_(Piano).mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/FF6epitaph_piano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Ff7-One_Winged.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ff7-mainmidi.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Rydia_pc.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ff4-fight1.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Still_Alive-1.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/goldsaucer.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/relmstheme-piano.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/FFX_-_Ending_Theme_(Piano_Version)_-_by_Angel_FF.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/In_Zanarkand.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/FF3_Third_Phase_Final_(Piano).mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Kingdom_Hearts_Dearly_Beloved.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/FFVII_BATTLE.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/caitsith.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/ff4-town.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/dayafter.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Gold_Silver_Rival_Battle.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Ff7-Jenova_Absolute.mid\n",
            "Parsing /content/Classical-Piano-Composer/midi_songs/Rachel_Piano_tempofix.mid\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cu_dnnlstm (CuDNNLSTM)       (None, 100, 512)          1054720   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 1024)              4202496   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 5,913,601\n",
            "Trainable params: 5,913,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 256)               256256    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               102500    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 100, 1)            0         \n",
            "=================================================================\n",
            "Total params: 1,022,820\n",
            "Trainable params: 1,019,236\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "0 [D loss: 0.696521, acc.: 42.19%] [G loss: 0.689285]\n",
            "1 [D loss: 0.670428, acc.: 65.62%] [G loss: 0.686695]\n",
            "2 [D loss: 0.637538, acc.: 62.50%] [G loss: 0.684536]\n",
            "3 [D loss: 0.578559, acc.: 71.88%] [G loss: 0.720896]\n",
            "4 [D loss: 0.378380, acc.: 87.50%] [G loss: 1.432564]\n",
            "5 [D loss: 0.267979, acc.: 93.75%] [G loss: 2.298923]\n",
            "6 [D loss: 0.137515, acc.: 96.88%] [G loss: 3.292562]\n",
            "7 [D loss: 0.509371, acc.: 90.62%] [G loss: 1.695561]\n",
            "8 [D loss: 0.300451, acc.: 92.19%] [G loss: 1.335008]\n",
            "9 [D loss: 0.333850, acc.: 90.62%] [G loss: 2.065949]\n",
            "10 [D loss: 0.237909, acc.: 90.62%] [G loss: 3.054235]\n",
            "11 [D loss: 0.233520, acc.: 93.75%] [G loss: 2.838232]\n",
            "12 [D loss: 0.052941, acc.: 98.44%] [G loss: 3.241358]\n",
            "13 [D loss: 0.467243, acc.: 84.38%] [G loss: 1.715106]\n",
            "14 [D loss: 0.163638, acc.: 95.31%] [G loss: 1.896895]\n",
            "15 [D loss: 0.518021, acc.: 87.50%] [G loss: 1.532698]\n",
            "16 [D loss: 0.324745, acc.: 90.62%] [G loss: 1.421454]\n",
            "17 [D loss: 0.273738, acc.: 93.75%] [G loss: 2.070760]\n",
            "18 [D loss: 0.423480, acc.: 87.50%] [G loss: 1.903968]\n",
            "19 [D loss: 0.217631, acc.: 93.75%] [G loss: 2.455996]\n",
            "20 [D loss: 0.281108, acc.: 89.06%] [G loss: 2.417207]\n",
            "21 [D loss: 0.353749, acc.: 84.38%] [G loss: 1.782860]\n",
            "22 [D loss: 0.311169, acc.: 93.75%] [G loss: 2.070059]\n",
            "23 [D loss: 0.303728, acc.: 90.62%] [G loss: 2.274400]\n",
            "24 [D loss: 0.224165, acc.: 92.19%] [G loss: 2.282359]\n",
            "25 [D loss: 0.255896, acc.: 90.62%] [G loss: 2.423230]\n",
            "26 [D loss: 0.289847, acc.: 92.19%] [G loss: 2.157409]\n",
            "27 [D loss: 0.300547, acc.: 89.06%] [G loss: 1.921534]\n",
            "28 [D loss: 0.161934, acc.: 96.88%] [G loss: 2.340517]\n",
            "29 [D loss: 0.315188, acc.: 85.94%] [G loss: 2.407168]\n",
            "30 [D loss: 0.200833, acc.: 93.75%] [G loss: 2.374071]\n",
            "31 [D loss: 0.325036, acc.: 89.06%] [G loss: 2.065649]\n",
            "32 [D loss: 0.394727, acc.: 85.94%] [G loss: 1.884770]\n",
            "33 [D loss: 0.360522, acc.: 84.38%] [G loss: 1.717340]\n",
            "34 [D loss: 0.243478, acc.: 89.06%] [G loss: 1.977506]\n",
            "35 [D loss: 0.225757, acc.: 90.62%] [G loss: 2.557493]\n",
            "36 [D loss: 0.475333, acc.: 82.81%] [G loss: 1.824829]\n",
            "37 [D loss: 0.269854, acc.: 92.19%] [G loss: 1.910883]\n",
            "38 [D loss: 0.482975, acc.: 82.81%] [G loss: 1.725443]\n",
            "39 [D loss: 0.383286, acc.: 81.25%] [G loss: 1.767924]\n",
            "40 [D loss: 0.527786, acc.: 79.69%] [G loss: 1.635518]\n",
            "41 [D loss: 0.310414, acc.: 90.62%] [G loss: 1.777283]\n",
            "42 [D loss: 0.322817, acc.: 85.94%] [G loss: 1.939484]\n",
            "43 [D loss: 0.256668, acc.: 90.62%] [G loss: 1.990052]\n",
            "44 [D loss: 0.516259, acc.: 79.69%] [G loss: 1.692648]\n",
            "45 [D loss: 0.343089, acc.: 87.50%] [G loss: 1.738670]\n",
            "46 [D loss: 0.276908, acc.: 89.06%] [G loss: 1.930284]\n",
            "47 [D loss: 0.385702, acc.: 84.38%] [G loss: 1.964913]\n",
            "48 [D loss: 0.343302, acc.: 81.25%] [G loss: 2.030339]\n",
            "49 [D loss: 0.447872, acc.: 85.94%] [G loss: 1.755087]\n",
            "50 [D loss: 0.348840, acc.: 85.94%] [G loss: 1.740920]\n",
            "51 [D loss: 0.306457, acc.: 84.38%] [G loss: 1.832431]\n",
            "52 [D loss: 0.327311, acc.: 89.06%] [G loss: 1.997900]\n",
            "53 [D loss: 0.385432, acc.: 85.94%] [G loss: 1.888976]\n",
            "54 [D loss: 0.304030, acc.: 90.62%] [G loss: 1.949194]\n",
            "55 [D loss: 0.453160, acc.: 76.56%] [G loss: 1.821439]\n",
            "56 [D loss: 0.277048, acc.: 92.19%] [G loss: 2.039733]\n",
            "57 [D loss: 0.350863, acc.: 82.81%] [G loss: 2.038253]\n",
            "58 [D loss: 0.331628, acc.: 85.94%] [G loss: 2.098974]\n",
            "59 [D loss: 0.292167, acc.: 90.62%] [G loss: 2.133762]\n",
            "60 [D loss: 0.233426, acc.: 95.31%] [G loss: 2.668756]\n",
            "61 [D loss: 0.327363, acc.: 89.06%] [G loss: 2.244256]\n",
            "62 [D loss: 0.432285, acc.: 82.81%] [G loss: 1.822639]\n",
            "63 [D loss: 0.238506, acc.: 90.62%] [G loss: 1.983163]\n",
            "64 [D loss: 0.408718, acc.: 81.25%] [G loss: 1.711335]\n",
            "65 [D loss: 0.318664, acc.: 89.06%] [G loss: 1.866838]\n",
            "66 [D loss: 0.486485, acc.: 78.12%] [G loss: 1.589475]\n",
            "67 [D loss: 0.294767, acc.: 92.19%] [G loss: 1.770955]\n",
            "68 [D loss: 0.317380, acc.: 87.50%] [G loss: 1.964953]\n",
            "69 [D loss: 0.521992, acc.: 78.12%] [G loss: 1.660141]\n",
            "70 [D loss: 0.443562, acc.: 82.81%] [G loss: 1.438528]\n",
            "71 [D loss: 0.443542, acc.: 79.69%] [G loss: 1.368621]\n",
            "72 [D loss: 0.379442, acc.: 84.38%] [G loss: 1.493008]\n",
            "73 [D loss: 0.315235, acc.: 89.06%] [G loss: 1.812161]\n",
            "74 [D loss: 0.410079, acc.: 81.25%] [G loss: 1.788970]\n",
            "75 [D loss: 0.353605, acc.: 81.25%] [G loss: 1.851867]\n",
            "76 [D loss: 0.317158, acc.: 87.50%] [G loss: 1.971921]\n",
            "77 [D loss: 0.273950, acc.: 90.62%] [G loss: 2.367285]\n",
            "78 [D loss: 0.556570, acc.: 78.12%] [G loss: 1.752321]\n",
            "79 [D loss: 0.293717, acc.: 90.62%] [G loss: 1.915710]\n",
            "80 [D loss: 0.259596, acc.: 90.62%] [G loss: 2.632041]\n",
            "81 [D loss: 0.389896, acc.: 81.25%] [G loss: 2.207992]\n",
            "82 [D loss: 0.442801, acc.: 79.69%] [G loss: 1.865911]\n",
            "83 [D loss: 0.413130, acc.: 85.94%] [G loss: 1.757934]\n",
            "84 [D loss: 0.273164, acc.: 89.06%] [G loss: 1.855604]\n",
            "85 [D loss: 0.409836, acc.: 81.25%] [G loss: 1.729687]\n",
            "86 [D loss: 0.518567, acc.: 76.56%] [G loss: 1.403921]\n",
            "87 [D loss: 0.347903, acc.: 89.06%] [G loss: 1.526857]\n",
            "88 [D loss: 0.414237, acc.: 82.81%] [G loss: 1.538979]\n",
            "89 [D loss: 0.410463, acc.: 85.94%] [G loss: 1.559695]\n",
            "90 [D loss: 0.257200, acc.: 87.50%] [G loss: 1.776017]\n",
            "91 [D loss: 0.368361, acc.: 82.81%] [G loss: 1.829429]\n",
            "92 [D loss: 0.481603, acc.: 82.81%] [G loss: 1.729132]\n",
            "93 [D loss: 0.319658, acc.: 82.81%] [G loss: 1.768979]\n",
            "94 [D loss: 0.507535, acc.: 79.69%] [G loss: 1.449444]\n",
            "95 [D loss: 0.436146, acc.: 81.25%] [G loss: 1.360696]\n",
            "96 [D loss: 0.353376, acc.: 84.38%] [G loss: 1.432779]\n",
            "97 [D loss: 0.462853, acc.: 79.69%] [G loss: 1.408335]\n",
            "98 [D loss: 0.336268, acc.: 81.25%] [G loss: 1.583262]\n",
            "99 [D loss: 0.416378, acc.: 79.69%] [G loss: 1.705547]\n",
            "100 [D loss: 0.421001, acc.: 82.81%] [G loss: 1.568594]\n",
            "101 [D loss: 0.541866, acc.: 82.81%] [G loss: 1.401184]\n",
            "102 [D loss: 0.433910, acc.: 78.12%] [G loss: 1.512607]\n",
            "103 [D loss: 0.508987, acc.: 71.88%] [G loss: 1.507743]\n",
            "104 [D loss: 0.442376, acc.: 82.81%] [G loss: 1.529318]\n",
            "105 [D loss: 0.621668, acc.: 64.06%] [G loss: 1.330415]\n",
            "106 [D loss: 0.653058, acc.: 65.62%] [G loss: 1.136601]\n",
            "107 [D loss: 0.479711, acc.: 79.69%] [G loss: 1.246762]\n",
            "108 [D loss: 0.507585, acc.: 70.31%] [G loss: 1.272927]\n",
            "109 [D loss: 0.391478, acc.: 81.25%] [G loss: 1.553807]\n",
            "110 [D loss: 0.653937, acc.: 67.19%] [G loss: 1.522374]\n",
            "111 [D loss: 0.714570, acc.: 62.50%] [G loss: 1.215307]\n",
            "112 [D loss: 0.529695, acc.: 75.00%] [G loss: 1.185210]\n",
            "113 [D loss: 0.486349, acc.: 78.12%] [G loss: 1.259216]\n",
            "114 [D loss: 0.419202, acc.: 78.12%] [G loss: 1.669586]\n",
            "115 [D loss: 0.477004, acc.: 81.25%] [G loss: 1.541010]\n",
            "116 [D loss: 0.427815, acc.: 78.12%] [G loss: 1.817384]\n",
            "117 [D loss: 0.480465, acc.: 78.12%] [G loss: 1.645784]\n",
            "118 [D loss: 0.600935, acc.: 73.44%] [G loss: 1.329419]\n",
            "119 [D loss: 0.448035, acc.: 76.56%] [G loss: 1.457909]\n",
            "120 [D loss: 0.671250, acc.: 64.06%] [G loss: 1.186992]\n",
            "121 [D loss: 0.570239, acc.: 68.75%] [G loss: 1.215400]\n",
            "122 [D loss: 0.561136, acc.: 71.88%] [G loss: 1.209666]\n",
            "123 [D loss: 0.518729, acc.: 76.56%] [G loss: 1.435830]\n",
            "124 [D loss: 0.427960, acc.: 76.56%] [G loss: 1.808523]\n",
            "125 [D loss: 0.466224, acc.: 79.69%] [G loss: 1.714628]\n",
            "126 [D loss: 0.468968, acc.: 81.25%] [G loss: 1.442912]\n",
            "127 [D loss: 0.463314, acc.: 76.56%] [G loss: 1.602013]\n",
            "128 [D loss: 0.488227, acc.: 76.56%] [G loss: 1.672824]\n",
            "129 [D loss: 0.561234, acc.: 76.56%] [G loss: 1.401078]\n",
            "130 [D loss: 0.647402, acc.: 64.06%] [G loss: 1.168774]\n",
            "131 [D loss: 0.558244, acc.: 67.19%] [G loss: 1.224698]\n",
            "132 [D loss: 0.525266, acc.: 73.44%] [G loss: 1.270599]\n",
            "133 [D loss: 0.531569, acc.: 76.56%] [G loss: 1.352179]\n",
            "134 [D loss: 0.354293, acc.: 85.94%] [G loss: 1.908354]\n",
            "135 [D loss: 0.531568, acc.: 78.12%] [G loss: 1.660774]\n",
            "136 [D loss: 0.600213, acc.: 75.00%] [G loss: 1.448840]\n",
            "137 [D loss: 0.593817, acc.: 73.44%] [G loss: 1.133721]\n",
            "138 [D loss: 0.555284, acc.: 73.44%] [G loss: 1.230416]\n",
            "139 [D loss: 0.430324, acc.: 82.81%] [G loss: 1.695577]\n",
            "140 [D loss: 0.623410, acc.: 67.19%] [G loss: 1.655255]\n",
            "141 [D loss: 0.505242, acc.: 76.56%] [G loss: 1.436092]\n",
            "142 [D loss: 0.443075, acc.: 81.25%] [G loss: 1.650151]\n",
            "143 [D loss: 0.575632, acc.: 71.88%] [G loss: 1.907372]\n",
            "144 [D loss: 0.370199, acc.: 82.81%] [G loss: 2.516911]\n",
            "145 [D loss: 0.630381, acc.: 78.12%] [G loss: 1.310482]\n",
            "146 [D loss: 0.401142, acc.: 87.50%] [G loss: 1.551173]\n",
            "147 [D loss: 0.643182, acc.: 64.06%] [G loss: 1.281444]\n",
            "148 [D loss: 0.560652, acc.: 71.88%] [G loss: 1.121399]\n",
            "149 [D loss: 0.488777, acc.: 79.69%] [G loss: 1.227548]\n",
            "150 [D loss: 0.465038, acc.: 78.12%] [G loss: 1.571758]\n",
            "151 [D loss: 0.503874, acc.: 79.69%] [G loss: 1.635530]\n",
            "152 [D loss: 0.575709, acc.: 70.31%] [G loss: 1.371674]\n",
            "153 [D loss: 0.381987, acc.: 90.62%] [G loss: 1.532269]\n",
            "154 [D loss: 0.487940, acc.: 79.69%] [G loss: 1.647353]\n",
            "155 [D loss: 0.561105, acc.: 75.00%] [G loss: 1.290444]\n",
            "156 [D loss: 0.518938, acc.: 75.00%] [G loss: 1.350979]\n",
            "157 [D loss: 0.479470, acc.: 79.69%] [G loss: 1.516018]\n",
            "158 [D loss: 0.360830, acc.: 85.94%] [G loss: 2.132622]\n",
            "159 [D loss: 0.518146, acc.: 79.69%] [G loss: 2.235929]\n",
            "160 [D loss: 0.600144, acc.: 75.00%] [G loss: 1.329372]\n",
            "161 [D loss: 0.481997, acc.: 76.56%] [G loss: 1.413149]\n",
            "162 [D loss: 0.586426, acc.: 67.19%] [G loss: 1.355285]\n",
            "163 [D loss: 0.536996, acc.: 76.56%] [G loss: 1.337698]\n",
            "164 [D loss: 0.476540, acc.: 79.69%] [G loss: 1.562245]\n",
            "165 [D loss: 0.562003, acc.: 70.31%] [G loss: 1.626933]\n",
            "166 [D loss: 0.520820, acc.: 78.12%] [G loss: 1.359353]\n",
            "167 [D loss: 0.481377, acc.: 79.69%] [G loss: 1.385445]\n",
            "168 [D loss: 0.491413, acc.: 79.69%] [G loss: 1.472413]\n",
            "169 [D loss: 0.536557, acc.: 73.44%] [G loss: 1.265304]\n",
            "170 [D loss: 0.477370, acc.: 78.12%] [G loss: 1.230591]\n",
            "171 [D loss: 0.529970, acc.: 75.00%] [G loss: 1.383765]\n",
            "172 [D loss: 0.546839, acc.: 76.56%] [G loss: 1.321746]\n",
            "173 [D loss: 0.589911, acc.: 68.75%] [G loss: 1.383760]\n",
            "174 [D loss: 0.565710, acc.: 70.31%] [G loss: 1.533894]\n",
            "175 [D loss: 0.455669, acc.: 78.12%] [G loss: 1.886492]\n",
            "176 [D loss: 0.483152, acc.: 81.25%] [G loss: 2.017556]\n",
            "177 [D loss: 0.546071, acc.: 71.88%] [G loss: 1.341495]\n",
            "178 [D loss: 0.596278, acc.: 71.88%] [G loss: 1.106810]\n",
            "179 [D loss: 0.670539, acc.: 57.81%] [G loss: 1.113581]\n",
            "180 [D loss: 0.562649, acc.: 70.31%] [G loss: 1.243606]\n",
            "181 [D loss: 0.601334, acc.: 68.75%] [G loss: 1.243258]\n",
            "182 [D loss: 0.458912, acc.: 81.25%] [G loss: 1.540542]\n",
            "183 [D loss: 0.569822, acc.: 73.44%] [G loss: 1.776047]\n",
            "184 [D loss: 0.544537, acc.: 76.56%] [G loss: 1.435702]\n",
            "185 [D loss: 0.625816, acc.: 65.62%] [G loss: 1.189601]\n",
            "186 [D loss: 0.556913, acc.: 75.00%] [G loss: 1.324185]\n",
            "187 [D loss: 0.527694, acc.: 76.56%] [G loss: 1.410582]\n",
            "188 [D loss: 0.565030, acc.: 73.44%] [G loss: 1.148346]\n",
            "189 [D loss: 0.555724, acc.: 70.31%] [G loss: 1.196840]\n",
            "190 [D loss: 0.572079, acc.: 75.00%] [G loss: 1.229597]\n",
            "191 [D loss: 0.663435, acc.: 65.62%] [G loss: 1.076278]\n",
            "192 [D loss: 0.578185, acc.: 64.06%] [G loss: 1.084514]\n",
            "193 [D loss: 0.631451, acc.: 59.38%] [G loss: 1.136072]\n",
            "194 [D loss: 0.578614, acc.: 71.88%] [G loss: 1.277928]\n",
            "195 [D loss: 0.650755, acc.: 59.38%] [G loss: 1.270203]\n",
            "196 [D loss: 0.615531, acc.: 65.62%] [G loss: 1.224549]\n",
            "197 [D loss: 0.659235, acc.: 62.50%] [G loss: 1.117184]\n",
            "198 [D loss: 0.581686, acc.: 68.75%] [G loss: 1.141841]\n",
            "199 [D loss: 0.542393, acc.: 75.00%] [G loss: 1.427585]\n",
            "200 [D loss: 0.642227, acc.: 65.62%] [G loss: 1.205773]\n",
            "201 [D loss: 0.662013, acc.: 60.94%] [G loss: 1.096961]\n",
            "202 [D loss: 0.594312, acc.: 70.31%] [G loss: 1.070978]\n",
            "203 [D loss: 0.635042, acc.: 68.75%] [G loss: 1.095460]\n",
            "204 [D loss: 0.583864, acc.: 70.31%] [G loss: 1.378758]\n",
            "205 [D loss: 0.486905, acc.: 76.56%] [G loss: 1.528401]\n",
            "206 [D loss: 0.646727, acc.: 68.75%] [G loss: 1.408293]\n",
            "207 [D loss: 0.558490, acc.: 73.44%] [G loss: 1.377813]\n",
            "208 [D loss: 0.634580, acc.: 67.19%] [G loss: 1.178207]\n",
            "209 [D loss: 0.605339, acc.: 64.06%] [G loss: 1.144921]\n",
            "210 [D loss: 0.564596, acc.: 73.44%] [G loss: 1.408110]\n",
            "211 [D loss: 0.595680, acc.: 67.19%] [G loss: 1.291039]\n",
            "212 [D loss: 0.618182, acc.: 65.62%] [G loss: 1.143706]\n",
            "213 [D loss: 0.517028, acc.: 73.44%] [G loss: 1.308792]\n",
            "214 [D loss: 0.628995, acc.: 67.19%] [G loss: 1.049353]\n",
            "215 [D loss: 0.422532, acc.: 84.38%] [G loss: 1.490392]\n",
            "216 [D loss: 0.508595, acc.: 79.69%] [G loss: 1.732534]\n",
            "217 [D loss: 0.896415, acc.: 34.38%] [G loss: 1.384886]\n",
            "218 [D loss: 0.801672, acc.: 43.75%] [G loss: 0.802285]\n",
            "219 [D loss: 0.646838, acc.: 68.75%] [G loss: 0.796698]\n",
            "220 [D loss: 0.656123, acc.: 57.81%] [G loss: 0.838975]\n",
            "221 [D loss: 0.691030, acc.: 54.69%] [G loss: 0.824382]\n",
            "222 [D loss: 0.626098, acc.: 62.50%] [G loss: 0.895509]\n",
            "223 [D loss: 0.569726, acc.: 67.19%] [G loss: 1.091518]\n",
            "224 [D loss: 0.612538, acc.: 67.19%] [G loss: 1.089387]\n",
            "225 [D loss: 0.564340, acc.: 70.31%] [G loss: 1.169782]\n",
            "226 [D loss: 0.638532, acc.: 59.38%] [G loss: 1.107148]\n",
            "227 [D loss: 0.652156, acc.: 64.06%] [G loss: 0.935897]\n",
            "228 [D loss: 0.621393, acc.: 67.19%] [G loss: 0.963064]\n",
            "229 [D loss: 0.728937, acc.: 48.44%] [G loss: 0.790445]\n",
            "230 [D loss: 0.613148, acc.: 68.75%] [G loss: 0.812337]\n",
            "231 [D loss: 0.616654, acc.: 65.62%] [G loss: 0.874262]\n",
            "232 [D loss: 0.589593, acc.: 70.31%] [G loss: 0.939136]\n",
            "233 [D loss: 0.549614, acc.: 70.31%] [G loss: 1.107087]\n",
            "234 [D loss: 0.667661, acc.: 60.94%] [G loss: 0.895305]\n",
            "235 [D loss: 0.602161, acc.: 67.19%] [G loss: 0.925009]\n",
            "236 [D loss: 0.582575, acc.: 68.75%] [G loss: 1.057796]\n",
            "237 [D loss: 0.599681, acc.: 71.88%] [G loss: 0.966181]\n",
            "238 [D loss: 0.613239, acc.: 68.75%] [G loss: 1.066099]\n",
            "239 [D loss: 0.715491, acc.: 50.00%] [G loss: 0.883970]\n",
            "240 [D loss: 0.645542, acc.: 64.06%] [G loss: 0.877571]\n",
            "241 [D loss: 0.626131, acc.: 67.19%] [G loss: 0.950323]\n",
            "242 [D loss: 0.711494, acc.: 53.12%] [G loss: 0.863595]\n",
            "243 [D loss: 0.635585, acc.: 62.50%] [G loss: 0.876910]\n",
            "244 [D loss: 0.627310, acc.: 68.75%] [G loss: 0.978253]\n",
            "245 [D loss: 0.586261, acc.: 73.44%] [G loss: 1.062714]\n",
            "246 [D loss: 0.572423, acc.: 73.44%] [G loss: 1.097151]\n",
            "247 [D loss: 0.636614, acc.: 65.62%] [G loss: 0.980302]\n",
            "248 [D loss: 0.529869, acc.: 71.88%] [G loss: 1.036362]\n",
            "249 [D loss: 0.554834, acc.: 70.31%] [G loss: 1.185024]\n",
            "250 [D loss: 0.611592, acc.: 64.06%] [G loss: 1.056441]\n",
            "251 [D loss: 0.623376, acc.: 70.31%] [G loss: 1.069417]\n",
            "252 [D loss: 0.598331, acc.: 68.75%] [G loss: 1.029157]\n",
            "253 [D loss: 0.629575, acc.: 64.06%] [G loss: 1.089514]\n",
            "254 [D loss: 0.631985, acc.: 65.62%] [G loss: 1.083932]\n",
            "255 [D loss: 0.665049, acc.: 54.69%] [G loss: 0.848876]\n",
            "256 [D loss: 0.709869, acc.: 54.69%] [G loss: 0.815009]\n",
            "257 [D loss: 0.554536, acc.: 73.44%] [G loss: 0.843989]\n",
            "258 [D loss: 0.638367, acc.: 59.38%] [G loss: 0.920001]\n",
            "259 [D loss: 0.701303, acc.: 57.81%] [G loss: 0.825670]\n",
            "260 [D loss: 0.626880, acc.: 65.62%] [G loss: 0.856055]\n",
            "261 [D loss: 0.679299, acc.: 60.94%] [G loss: 0.829287]\n",
            "262 [D loss: 0.706068, acc.: 54.69%] [G loss: 0.804230]\n",
            "263 [D loss: 0.639882, acc.: 59.38%] [G loss: 0.820976]\n",
            "264 [D loss: 0.600532, acc.: 68.75%] [G loss: 0.857009]\n",
            "265 [D loss: 0.651093, acc.: 64.06%] [G loss: 0.940266]\n",
            "266 [D loss: 0.553594, acc.: 78.12%] [G loss: 1.107274]\n",
            "267 [D loss: 0.648959, acc.: 73.44%] [G loss: 0.964588]\n",
            "268 [D loss: 0.729278, acc.: 56.25%] [G loss: 0.839380]\n",
            "269 [D loss: 0.637411, acc.: 71.88%] [G loss: 0.855888]\n",
            "270 [D loss: 0.580941, acc.: 67.19%] [G loss: 0.954233]\n",
            "271 [D loss: 0.663176, acc.: 62.50%] [G loss: 0.965081]\n",
            "272 [D loss: 0.623293, acc.: 65.62%] [G loss: 0.921582]\n",
            "273 [D loss: 0.720756, acc.: 57.81%] [G loss: 0.829872]\n",
            "274 [D loss: 0.626288, acc.: 64.06%] [G loss: 0.858635]\n",
            "275 [D loss: 0.670175, acc.: 59.38%] [G loss: 0.893665]\n",
            "276 [D loss: 0.656310, acc.: 59.38%] [G loss: 0.890176]\n",
            "277 [D loss: 0.618849, acc.: 65.62%] [G loss: 0.911704]\n",
            "278 [D loss: 0.666867, acc.: 62.50%] [G loss: 0.888578]\n",
            "279 [D loss: 0.693504, acc.: 57.81%] [G loss: 0.812588]\n",
            "280 [D loss: 0.700830, acc.: 56.25%] [G loss: 0.820352]\n",
            "281 [D loss: 0.658836, acc.: 56.25%] [G loss: 0.804185]\n",
            "282 [D loss: 0.610417, acc.: 68.75%] [G loss: 0.865754]\n",
            "283 [D loss: 0.635574, acc.: 56.25%] [G loss: 0.952003]\n",
            "284 [D loss: 0.616333, acc.: 64.06%] [G loss: 1.148768]\n",
            "285 [D loss: 0.669190, acc.: 68.75%] [G loss: 0.825797]\n",
            "286 [D loss: 0.625851, acc.: 64.06%] [G loss: 0.907022]\n",
            "287 [D loss: 0.592036, acc.: 65.62%] [G loss: 0.896378]\n",
            "288 [D loss: 0.671514, acc.: 54.69%] [G loss: 0.868823]\n",
            "289 [D loss: 0.645634, acc.: 51.56%] [G loss: 0.881274]\n",
            "290 [D loss: 0.665759, acc.: 51.56%] [G loss: 0.823234]\n",
            "291 [D loss: 0.691709, acc.: 57.81%] [G loss: 0.852856]\n",
            "292 [D loss: 0.671943, acc.: 53.12%] [G loss: 0.828874]\n",
            "293 [D loss: 0.624660, acc.: 64.06%] [G loss: 0.867917]\n",
            "294 [D loss: 0.656184, acc.: 59.38%] [G loss: 0.877175]\n",
            "295 [D loss: 0.640578, acc.: 64.06%] [G loss: 0.851290]\n",
            "296 [D loss: 0.623555, acc.: 64.06%] [G loss: 0.891640]\n",
            "297 [D loss: 0.701309, acc.: 62.50%] [G loss: 0.844073]\n",
            "298 [D loss: 0.705176, acc.: 48.44%] [G loss: 0.788101]\n",
            "299 [D loss: 0.725499, acc.: 46.88%] [G loss: 0.748652]\n",
            "300 [D loss: 0.671095, acc.: 57.81%] [G loss: 0.767658]\n",
            "301 [D loss: 0.673375, acc.: 53.12%] [G loss: 0.762111]\n",
            "302 [D loss: 0.644868, acc.: 60.94%] [G loss: 0.853982]\n",
            "303 [D loss: 0.690121, acc.: 53.12%] [G loss: 0.882909]\n",
            "304 [D loss: 0.677482, acc.: 57.81%] [G loss: 0.916327]\n",
            "305 [D loss: 0.720233, acc.: 57.81%] [G loss: 0.801297]\n",
            "306 [D loss: 0.645917, acc.: 73.44%] [G loss: 0.926448]\n",
            "307 [D loss: 0.702266, acc.: 54.69%] [G loss: 0.723516]\n",
            "308 [D loss: 0.696859, acc.: 57.81%] [G loss: 0.738717]\n",
            "309 [D loss: 0.642151, acc.: 68.75%] [G loss: 0.765651]\n",
            "310 [D loss: 0.707101, acc.: 51.56%] [G loss: 0.777336]\n",
            "311 [D loss: 0.676057, acc.: 56.25%] [G loss: 0.767563]\n",
            "312 [D loss: 0.689841, acc.: 59.38%] [G loss: 0.811442]\n",
            "313 [D loss: 0.657860, acc.: 64.06%] [G loss: 0.824802]\n",
            "314 [D loss: 0.670421, acc.: 56.25%] [G loss: 0.824250]\n",
            "315 [D loss: 0.694357, acc.: 50.00%] [G loss: 0.813244]\n",
            "316 [D loss: 0.697415, acc.: 50.00%] [G loss: 0.835059]\n",
            "317 [D loss: 0.695246, acc.: 48.44%] [G loss: 0.789230]\n",
            "318 [D loss: 0.663100, acc.: 54.69%] [G loss: 0.785230]\n",
            "319 [D loss: 0.674860, acc.: 50.00%] [G loss: 0.792854]\n",
            "320 [D loss: 0.702305, acc.: 51.56%] [G loss: 0.800321]\n",
            "321 [D loss: 0.645958, acc.: 64.06%] [G loss: 0.802845]\n",
            "322 [D loss: 0.660595, acc.: 62.50%] [G loss: 0.817144]\n",
            "323 [D loss: 0.647709, acc.: 68.75%] [G loss: 0.865759]\n",
            "324 [D loss: 0.664119, acc.: 60.94%] [G loss: 0.920399]\n",
            "325 [D loss: 0.677491, acc.: 57.81%] [G loss: 0.830684]\n",
            "326 [D loss: 0.698594, acc.: 56.25%] [G loss: 0.828956]\n",
            "327 [D loss: 0.641385, acc.: 67.19%] [G loss: 0.819798]\n",
            "328 [D loss: 0.700521, acc.: 50.00%] [G loss: 0.760691]\n",
            "329 [D loss: 0.665442, acc.: 60.94%] [G loss: 0.772585]\n",
            "330 [D loss: 0.717197, acc.: 43.75%] [G loss: 0.720568]\n",
            "331 [D loss: 0.659778, acc.: 65.62%] [G loss: 0.720751]\n",
            "332 [D loss: 0.655181, acc.: 62.50%] [G loss: 0.732487]\n",
            "333 [D loss: 0.654360, acc.: 62.50%] [G loss: 0.753018]\n",
            "334 [D loss: 0.658399, acc.: 60.94%] [G loss: 0.759558]\n",
            "335 [D loss: 0.682080, acc.: 46.88%] [G loss: 0.738942]\n",
            "336 [D loss: 0.654935, acc.: 59.38%] [G loss: 0.729646]\n",
            "337 [D loss: 0.672205, acc.: 57.81%] [G loss: 0.745793]\n",
            "338 [D loss: 0.703427, acc.: 57.81%] [G loss: 0.749030]\n",
            "339 [D loss: 0.655881, acc.: 59.38%] [G loss: 0.762306]\n",
            "340 [D loss: 0.697667, acc.: 48.44%] [G loss: 0.724273]\n",
            "341 [D loss: 0.657746, acc.: 59.38%] [G loss: 0.733784]\n",
            "342 [D loss: 0.656395, acc.: 56.25%] [G loss: 0.735606]\n",
            "343 [D loss: 0.662133, acc.: 57.81%] [G loss: 0.727194]\n",
            "344 [D loss: 0.716412, acc.: 50.00%] [G loss: 0.717298]\n",
            "345 [D loss: 0.664003, acc.: 51.56%] [G loss: 0.725181]\n",
            "346 [D loss: 0.693434, acc.: 48.44%] [G loss: 0.709844]\n",
            "347 [D loss: 0.702171, acc.: 45.31%] [G loss: 0.735296]\n",
            "348 [D loss: 0.696291, acc.: 48.44%] [G loss: 0.730499]\n",
            "349 [D loss: 0.645932, acc.: 70.31%] [G loss: 0.732838]\n",
            "350 [D loss: 0.714541, acc.: 42.19%] [G loss: 0.740452]\n",
            "351 [D loss: 0.688167, acc.: 57.81%] [G loss: 0.743169]\n",
            "352 [D loss: 0.709010, acc.: 46.88%] [G loss: 0.729563]\n",
            "353 [D loss: 0.699745, acc.: 48.44%] [G loss: 0.745391]\n",
            "354 [D loss: 0.708843, acc.: 43.75%] [G loss: 0.729544]\n",
            "355 [D loss: 0.699065, acc.: 56.25%] [G loss: 0.712538]\n",
            "356 [D loss: 0.692291, acc.: 48.44%] [G loss: 0.723069]\n",
            "357 [D loss: 0.682944, acc.: 62.50%] [G loss: 0.718484]\n",
            "358 [D loss: 0.684113, acc.: 56.25%] [G loss: 0.709084]\n",
            "359 [D loss: 0.661220, acc.: 64.06%] [G loss: 0.712464]\n",
            "360 [D loss: 0.682966, acc.: 54.69%] [G loss: 0.718424]\n",
            "361 [D loss: 0.670696, acc.: 62.50%] [G loss: 0.712490]\n",
            "362 [D loss: 0.683294, acc.: 54.69%] [G loss: 0.708025]\n",
            "363 [D loss: 0.669606, acc.: 59.38%] [G loss: 0.710200]\n",
            "364 [D loss: 0.684067, acc.: 53.12%] [G loss: 0.690262]\n",
            "365 [D loss: 0.670636, acc.: 57.81%] [G loss: 0.695682]\n",
            "366 [D loss: 0.707466, acc.: 46.88%] [G loss: 0.707806]\n",
            "367 [D loss: 0.666029, acc.: 59.38%] [G loss: 0.699692]\n",
            "368 [D loss: 0.671514, acc.: 60.94%] [G loss: 0.690329]\n",
            "369 [D loss: 0.680431, acc.: 57.81%] [G loss: 0.704733]\n",
            "370 [D loss: 0.679556, acc.: 56.25%] [G loss: 0.703651]\n",
            "371 [D loss: 0.711573, acc.: 51.56%] [G loss: 0.707188]\n",
            "372 [D loss: 0.718827, acc.: 40.62%] [G loss: 0.704028]\n",
            "373 [D loss: 0.681091, acc.: 57.81%] [G loss: 0.703656]\n",
            "374 [D loss: 0.695594, acc.: 48.44%] [G loss: 0.704851]\n",
            "375 [D loss: 0.699412, acc.: 48.44%] [G loss: 0.693473]\n",
            "376 [D loss: 0.693298, acc.: 42.19%] [G loss: 0.695876]\n",
            "377 [D loss: 0.720848, acc.: 43.75%] [G loss: 0.700456]\n",
            "378 [D loss: 0.689758, acc.: 50.00%] [G loss: 0.700685]\n",
            "379 [D loss: 0.688664, acc.: 56.25%] [G loss: 0.698387]\n",
            "380 [D loss: 0.691899, acc.: 51.56%] [G loss: 0.695327]\n",
            "381 [D loss: 0.690583, acc.: 51.56%] [G loss: 0.691494]\n",
            "382 [D loss: 0.698214, acc.: 53.12%] [G loss: 0.692014]\n",
            "383 [D loss: 0.686251, acc.: 51.56%] [G loss: 0.694495]\n",
            "384 [D loss: 0.679168, acc.: 56.25%] [G loss: 0.694852]\n",
            "385 [D loss: 0.687400, acc.: 56.25%] [G loss: 0.693118]\n",
            "386 [D loss: 0.703403, acc.: 51.56%] [G loss: 0.695203]\n",
            "387 [D loss: 0.702744, acc.: 43.75%] [G loss: 0.693426]\n",
            "388 [D loss: 0.704648, acc.: 42.19%] [G loss: 0.700882]\n",
            "389 [D loss: 0.688653, acc.: 62.50%] [G loss: 0.704607]\n",
            "390 [D loss: 0.689211, acc.: 48.44%] [G loss: 0.693503]\n",
            "391 [D loss: 0.706059, acc.: 39.06%] [G loss: 0.696178]\n",
            "392 [D loss: 0.696144, acc.: 50.00%] [G loss: 0.696998]\n",
            "393 [D loss: 0.685269, acc.: 54.69%] [G loss: 0.701249]\n",
            "394 [D loss: 0.690181, acc.: 56.25%] [G loss: 0.693197]\n",
            "395 [D loss: 0.713988, acc.: 34.38%] [G loss: 0.701189]\n",
            "396 [D loss: 0.689941, acc.: 46.88%] [G loss: 0.703198]\n",
            "397 [D loss: 0.682822, acc.: 65.62%] [G loss: 0.694017]\n",
            "398 [D loss: 0.687111, acc.: 51.56%] [G loss: 0.699174]\n",
            "399 [D loss: 0.697933, acc.: 46.88%] [G loss: 0.702875]\n",
            "400 [D loss: 0.689119, acc.: 51.56%] [G loss: 0.709431]\n",
            "401 [D loss: 0.682703, acc.: 53.12%] [G loss: 0.710038]\n",
            "402 [D loss: 0.681737, acc.: 57.81%] [G loss: 0.706544]\n",
            "403 [D loss: 0.697609, acc.: 48.44%] [G loss: 0.701426]\n",
            "404 [D loss: 0.714249, acc.: 43.75%] [G loss: 0.701819]\n",
            "405 [D loss: 0.696138, acc.: 43.75%] [G loss: 0.708772]\n",
            "406 [D loss: 0.699546, acc.: 45.31%] [G loss: 0.707535]\n",
            "407 [D loss: 0.694948, acc.: 50.00%] [G loss: 0.708975]\n",
            "408 [D loss: 0.699343, acc.: 34.38%] [G loss: 0.715242]\n",
            "409 [D loss: 0.687707, acc.: 60.94%] [G loss: 0.717861]\n",
            "410 [D loss: 0.699733, acc.: 34.38%] [G loss: 0.712238]\n",
            "411 [D loss: 0.685011, acc.: 59.38%] [G loss: 0.714678]\n",
            "412 [D loss: 0.685475, acc.: 53.12%] [G loss: 0.718165]\n",
            "413 [D loss: 0.683895, acc.: 50.00%] [G loss: 0.716558]\n",
            "414 [D loss: 0.688228, acc.: 60.94%] [G loss: 0.719217]\n",
            "415 [D loss: 0.685088, acc.: 48.44%] [G loss: 0.718193]\n",
            "416 [D loss: 0.690789, acc.: 46.88%] [G loss: 0.718166]\n",
            "417 [D loss: 0.682543, acc.: 62.50%] [G loss: 0.724013]\n",
            "418 [D loss: 0.690206, acc.: 59.38%] [G loss: 0.746697]\n",
            "419 [D loss: 0.683054, acc.: 54.69%] [G loss: 0.733543]\n",
            "420 [D loss: 0.686883, acc.: 54.69%] [G loss: 0.751280]\n",
            "421 [D loss: 0.679595, acc.: 60.94%] [G loss: 0.723420]\n",
            "422 [D loss: 0.716033, acc.: 40.62%] [G loss: 0.711661]\n",
            "423 [D loss: 0.704585, acc.: 43.75%] [G loss: 0.712695]\n",
            "424 [D loss: 0.693594, acc.: 50.00%] [G loss: 0.709472]\n",
            "425 [D loss: 0.698099, acc.: 46.88%] [G loss: 0.711581]\n",
            "426 [D loss: 0.698293, acc.: 51.56%] [G loss: 0.707361]\n",
            "427 [D loss: 0.695958, acc.: 48.44%] [G loss: 0.710621]\n",
            "428 [D loss: 0.695823, acc.: 50.00%] [G loss: 0.710320]\n",
            "429 [D loss: 0.696965, acc.: 54.69%] [G loss: 0.702287]\n",
            "430 [D loss: 0.675600, acc.: 62.50%] [G loss: 0.695476]\n",
            "431 [D loss: 0.696619, acc.: 51.56%] [G loss: 0.695532]\n",
            "432 [D loss: 0.693788, acc.: 50.00%] [G loss: 0.699793]\n",
            "433 [D loss: 0.692642, acc.: 59.38%] [G loss: 0.698269]\n",
            "434 [D loss: 0.689548, acc.: 51.56%] [G loss: 0.690772]\n",
            "435 [D loss: 0.697608, acc.: 50.00%] [G loss: 0.685269]\n",
            "436 [D loss: 0.698967, acc.: 43.75%] [G loss: 0.684014]\n",
            "437 [D loss: 0.699569, acc.: 40.62%] [G loss: 0.685828]\n",
            "438 [D loss: 0.694349, acc.: 48.44%] [G loss: 0.680881]\n",
            "439 [D loss: 0.695379, acc.: 51.56%] [G loss: 0.687794]\n",
            "440 [D loss: 0.687994, acc.: 46.88%] [G loss: 0.694003]\n",
            "441 [D loss: 0.698588, acc.: 42.19%] [G loss: 0.685367]\n",
            "442 [D loss: 0.686337, acc.: 54.69%] [G loss: 0.676317]\n",
            "443 [D loss: 0.706281, acc.: 42.19%] [G loss: 0.682815]\n",
            "444 [D loss: 0.699364, acc.: 37.50%] [G loss: 0.680657]\n",
            "445 [D loss: 0.694095, acc.: 50.00%] [G loss: 0.684210]\n",
            "446 [D loss: 0.693507, acc.: 46.88%] [G loss: 0.684534]\n",
            "447 [D loss: 0.694794, acc.: 46.88%] [G loss: 0.687714]\n",
            "448 [D loss: 0.695289, acc.: 43.75%] [G loss: 0.681306]\n",
            "449 [D loss: 0.697631, acc.: 51.56%] [G loss: 0.677112]\n",
            "450 [D loss: 0.697430, acc.: 56.25%] [G loss: 0.678583]\n",
            "451 [D loss: 0.693859, acc.: 46.88%] [G loss: 0.687055]\n",
            "452 [D loss: 0.693871, acc.: 46.88%] [G loss: 0.685568]\n",
            "453 [D loss: 0.701153, acc.: 40.62%] [G loss: 0.687507]\n",
            "454 [D loss: 0.694431, acc.: 42.19%] [G loss: 0.687269]\n",
            "455 [D loss: 0.700489, acc.: 42.19%] [G loss: 0.686300]\n",
            "456 [D loss: 0.692201, acc.: 50.00%] [G loss: 0.687041]\n",
            "457 [D loss: 0.697089, acc.: 42.19%] [G loss: 0.684183]\n",
            "458 [D loss: 0.699002, acc.: 45.31%] [G loss: 0.684569]\n",
            "459 [D loss: 0.694124, acc.: 50.00%] [G loss: 0.685477]\n",
            "460 [D loss: 0.691998, acc.: 53.12%] [G loss: 0.683918]\n",
            "461 [D loss: 0.696535, acc.: 45.31%] [G loss: 0.687625]\n",
            "462 [D loss: 0.690605, acc.: 59.38%] [G loss: 0.684173]\n",
            "463 [D loss: 0.698173, acc.: 39.06%] [G loss: 0.684591]\n",
            "464 [D loss: 0.689252, acc.: 50.00%] [G loss: 0.684659]\n",
            "465 [D loss: 0.697631, acc.: 43.75%] [G loss: 0.688151]\n",
            "466 [D loss: 0.692345, acc.: 46.88%] [G loss: 0.689611]\n",
            "467 [D loss: 0.687822, acc.: 57.81%] [G loss: 0.689774]\n",
            "468 [D loss: 0.696361, acc.: 42.19%] [G loss: 0.687424]\n",
            "469 [D loss: 0.688517, acc.: 50.00%] [G loss: 0.686564]\n",
            "470 [D loss: 0.693591, acc.: 45.31%] [G loss: 0.687270]\n",
            "471 [D loss: 0.698652, acc.: 43.75%] [G loss: 0.685222]\n",
            "472 [D loss: 0.693512, acc.: 56.25%] [G loss: 0.687801]\n",
            "473 [D loss: 0.699939, acc.: 34.38%] [G loss: 0.688617]\n",
            "474 [D loss: 0.695356, acc.: 46.88%] [G loss: 0.689039]\n",
            "475 [D loss: 0.699243, acc.: 42.19%] [G loss: 0.691956]\n",
            "476 [D loss: 0.694892, acc.: 46.88%] [G loss: 0.691921]\n",
            "477 [D loss: 0.692162, acc.: 53.12%] [G loss: 0.695564]\n",
            "478 [D loss: 0.694659, acc.: 53.12%] [G loss: 0.693480]\n",
            "479 [D loss: 0.694439, acc.: 50.00%] [G loss: 0.692462]\n",
            "480 [D loss: 0.692194, acc.: 50.00%] [G loss: 0.696132]\n",
            "481 [D loss: 0.694765, acc.: 42.19%] [G loss: 0.699005]\n",
            "482 [D loss: 0.693911, acc.: 51.56%] [G loss: 0.698994]\n",
            "483 [D loss: 0.692233, acc.: 57.81%] [G loss: 0.701563]\n",
            "484 [D loss: 0.692965, acc.: 53.12%] [G loss: 0.697037]\n",
            "485 [D loss: 0.696237, acc.: 46.88%] [G loss: 0.696928]\n",
            "486 [D loss: 0.693563, acc.: 51.56%] [G loss: 0.698437]\n",
            "487 [D loss: 0.691916, acc.: 56.25%] [G loss: 0.700432]\n",
            "488 [D loss: 0.694554, acc.: 45.31%] [G loss: 0.698969]\n",
            "489 [D loss: 0.691094, acc.: 62.50%] [G loss: 0.701803]\n",
            "490 [D loss: 0.696191, acc.: 48.44%] [G loss: 0.704112]\n",
            "491 [D loss: 0.689245, acc.: 54.69%] [G loss: 0.703898]\n",
            "492 [D loss: 0.693169, acc.: 46.88%] [G loss: 0.703030]\n",
            "493 [D loss: 0.693242, acc.: 45.31%] [G loss: 0.697340]\n",
            "494 [D loss: 0.692598, acc.: 51.56%] [G loss: 0.703304]\n",
            "495 [D loss: 0.695854, acc.: 46.88%] [G loss: 0.700639]\n",
            "496 [D loss: 0.700855, acc.: 37.50%] [G loss: 0.702736]\n",
            "497 [D loss: 0.693112, acc.: 59.38%] [G loss: 0.708334]\n",
            "498 [D loss: 0.697844, acc.: 43.75%] [G loss: 0.705867]\n",
            "499 [D loss: 0.699445, acc.: 45.31%] [G loss: 0.706323]\n",
            "500 [D loss: 0.696776, acc.: 37.50%] [G loss: 0.699603]\n",
            "501 [D loss: 0.690876, acc.: 46.88%] [G loss: 0.696524]\n",
            "502 [D loss: 0.694089, acc.: 46.88%] [G loss: 0.701273]\n",
            "503 [D loss: 0.695035, acc.: 42.19%] [G loss: 0.697714]\n",
            "504 [D loss: 0.695018, acc.: 42.19%] [G loss: 0.694441]\n",
            "505 [D loss: 0.694205, acc.: 40.62%] [G loss: 0.695413]\n",
            "506 [D loss: 0.691502, acc.: 48.44%] [G loss: 0.693195]\n",
            "507 [D loss: 0.696518, acc.: 45.31%] [G loss: 0.695062]\n",
            "508 [D loss: 0.690054, acc.: 54.69%] [G loss: 0.694563]\n",
            "509 [D loss: 0.691511, acc.: 57.81%] [G loss: 0.697164]\n",
            "510 [D loss: 0.694195, acc.: 46.88%] [G loss: 0.695523]\n",
            "511 [D loss: 0.697067, acc.: 40.62%] [G loss: 0.694281]\n",
            "512 [D loss: 0.690083, acc.: 65.62%] [G loss: 0.691717]\n",
            "513 [D loss: 0.696516, acc.: 42.19%] [G loss: 0.692990]\n",
            "514 [D loss: 0.691440, acc.: 56.25%] [G loss: 0.692247]\n",
            "515 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.696949]\n",
            "516 [D loss: 0.693514, acc.: 53.12%] [G loss: 0.695575]\n",
            "517 [D loss: 0.689618, acc.: 62.50%] [G loss: 0.693338]\n",
            "518 [D loss: 0.693042, acc.: 51.56%] [G loss: 0.692555]\n",
            "519 [D loss: 0.694908, acc.: 39.06%] [G loss: 0.686567]\n",
            "520 [D loss: 0.697914, acc.: 40.62%] [G loss: 0.693313]\n",
            "521 [D loss: 0.690374, acc.: 56.25%] [G loss: 0.696146]\n",
            "522 [D loss: 0.700111, acc.: 43.75%] [G loss: 0.695853]\n",
            "523 [D loss: 0.696596, acc.: 34.38%] [G loss: 0.694643]\n",
            "524 [D loss: 0.695738, acc.: 43.75%] [G loss: 0.697093]\n",
            "525 [D loss: 0.696121, acc.: 40.62%] [G loss: 0.696491]\n",
            "526 [D loss: 0.693170, acc.: 56.25%] [G loss: 0.697625]\n",
            "527 [D loss: 0.693748, acc.: 50.00%] [G loss: 0.692631]\n",
            "528 [D loss: 0.692805, acc.: 51.56%] [G loss: 0.702368]\n",
            "529 [D loss: 0.693011, acc.: 46.88%] [G loss: 0.700964]\n",
            "530 [D loss: 0.690981, acc.: 48.44%] [G loss: 0.702436]\n",
            "531 [D loss: 0.695942, acc.: 48.44%] [G loss: 0.698237]\n",
            "532 [D loss: 0.694229, acc.: 50.00%] [G loss: 0.698488]\n",
            "533 [D loss: 0.697160, acc.: 40.62%] [G loss: 0.697428]\n",
            "534 [D loss: 0.694740, acc.: 45.31%] [G loss: 0.695067]\n",
            "535 [D loss: 0.696372, acc.: 50.00%] [G loss: 0.692726]\n",
            "536 [D loss: 0.690512, acc.: 50.00%] [G loss: 0.690052]\n",
            "537 [D loss: 0.693202, acc.: 57.81%] [G loss: 0.692413]\n",
            "538 [D loss: 0.694264, acc.: 45.31%] [G loss: 0.696058]\n",
            "539 [D loss: 0.690354, acc.: 54.69%] [G loss: 0.690956]\n",
            "540 [D loss: 0.696636, acc.: 42.19%] [G loss: 0.693256]\n",
            "541 [D loss: 0.694564, acc.: 48.44%] [G loss: 0.688657]\n",
            "542 [D loss: 0.692102, acc.: 53.12%] [G loss: 0.691326]\n",
            "543 [D loss: 0.692985, acc.: 56.25%] [G loss: 0.696176]\n",
            "544 [D loss: 0.693013, acc.: 42.19%] [G loss: 0.695140]\n",
            "545 [D loss: 0.690366, acc.: 53.12%] [G loss: 0.691817]\n",
            "546 [D loss: 0.688876, acc.: 56.25%] [G loss: 0.701466]\n",
            "547 [D loss: 0.700457, acc.: 46.88%] [G loss: 0.700152]\n",
            "548 [D loss: 0.690815, acc.: 54.69%] [G loss: 0.699661]\n",
            "549 [D loss: 0.688580, acc.: 59.38%] [G loss: 0.703011]\n",
            "550 [D loss: 0.696170, acc.: 48.44%] [G loss: 0.698151]\n",
            "551 [D loss: 0.678278, acc.: 65.62%] [G loss: 0.700966]\n",
            "552 [D loss: 0.694356, acc.: 53.12%] [G loss: 0.686367]\n",
            "553 [D loss: 0.736721, acc.: 34.38%] [G loss: 0.698931]\n",
            "554 [D loss: 0.697982, acc.: 42.19%] [G loss: 0.697913]\n",
            "555 [D loss: 0.693684, acc.: 46.88%] [G loss: 0.697529]\n",
            "556 [D loss: 0.690076, acc.: 60.94%] [G loss: 0.700624]\n",
            "557 [D loss: 0.695107, acc.: 43.75%] [G loss: 0.699343]\n",
            "558 [D loss: 0.694468, acc.: 45.31%] [G loss: 0.700074]\n",
            "559 [D loss: 0.696133, acc.: 42.19%] [G loss: 0.701106]\n",
            "560 [D loss: 0.691161, acc.: 60.94%] [G loss: 0.699910]\n",
            "561 [D loss: 0.695887, acc.: 40.62%] [G loss: 0.697885]\n",
            "562 [D loss: 0.697260, acc.: 46.88%] [G loss: 0.695179]\n",
            "563 [D loss: 0.695044, acc.: 46.88%] [G loss: 0.696738]\n",
            "564 [D loss: 0.693421, acc.: 50.00%] [G loss: 0.693298]\n",
            "565 [D loss: 0.692527, acc.: 53.12%] [G loss: 0.696108]\n",
            "566 [D loss: 0.694101, acc.: 42.19%] [G loss: 0.698629]\n",
            "567 [D loss: 0.694234, acc.: 50.00%] [G loss: 0.699060]\n",
            "568 [D loss: 0.694609, acc.: 48.44%] [G loss: 0.695376]\n",
            "569 [D loss: 0.694763, acc.: 43.75%] [G loss: 0.699232]\n",
            "570 [D loss: 0.691839, acc.: 53.12%] [G loss: 0.693062]\n",
            "571 [D loss: 0.696393, acc.: 32.81%] [G loss: 0.697010]\n",
            "572 [D loss: 0.693163, acc.: 54.69%] [G loss: 0.691765]\n",
            "573 [D loss: 0.692000, acc.: 53.12%] [G loss: 0.695488]\n",
            "574 [D loss: 0.693859, acc.: 39.06%] [G loss: 0.698179]\n",
            "575 [D loss: 0.694068, acc.: 51.56%] [G loss: 0.695531]\n",
            "576 [D loss: 0.696290, acc.: 42.19%] [G loss: 0.694101]\n",
            "577 [D loss: 0.694693, acc.: 46.88%] [G loss: 0.696571]\n",
            "578 [D loss: 0.690598, acc.: 53.12%] [G loss: 0.695432]\n",
            "579 [D loss: 0.692191, acc.: 54.69%] [G loss: 0.697967]\n",
            "580 [D loss: 0.694250, acc.: 46.88%] [G loss: 0.696563]\n",
            "581 [D loss: 0.693346, acc.: 42.19%] [G loss: 0.696849]\n",
            "582 [D loss: 0.693882, acc.: 48.44%] [G loss: 0.693216]\n",
            "583 [D loss: 0.693214, acc.: 48.44%] [G loss: 0.693693]\n",
            "584 [D loss: 0.695361, acc.: 40.62%] [G loss: 0.697090]\n",
            "585 [D loss: 0.693662, acc.: 48.44%] [G loss: 0.696023]\n",
            "586 [D loss: 0.693413, acc.: 50.00%] [G loss: 0.694616]\n",
            "587 [D loss: 0.690747, acc.: 54.69%] [G loss: 0.694812]\n",
            "588 [D loss: 0.693007, acc.: 51.56%] [G loss: 0.694627]\n",
            "589 [D loss: 0.694733, acc.: 43.75%] [G loss: 0.695892]\n",
            "590 [D loss: 0.693374, acc.: 43.75%] [G loss: 0.697530]\n",
            "591 [D loss: 0.695726, acc.: 43.75%] [G loss: 0.699295]\n",
            "592 [D loss: 0.695681, acc.: 45.31%] [G loss: 0.701273]\n",
            "593 [D loss: 0.691522, acc.: 54.69%] [G loss: 0.707698]\n",
            "594 [D loss: 0.694588, acc.: 40.62%] [G loss: 0.699165]\n",
            "595 [D loss: 0.694236, acc.: 37.50%] [G loss: 0.710206]\n",
            "596 [D loss: 0.694852, acc.: 45.31%] [G loss: 0.708326]\n",
            "597 [D loss: 0.693260, acc.: 50.00%] [G loss: 0.705657]\n",
            "598 [D loss: 0.692914, acc.: 48.44%] [G loss: 0.707371]\n",
            "599 [D loss: 0.692981, acc.: 46.88%] [G loss: 0.700856]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pnusahO9j0q",
        "outputId": "a076db6e-4ebc-41f2-e495-153cfbdfd02e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "gan.generate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: '0', 1: '0.1', 2: '0.1.5', 3: '0.1.6', 4: '0.2', 5: '0.2.3.7', 6: '0.2.4.7', 7: '0.2.5', 8: '0.2.6', 9: '0.2.7', 10: '0.3', 11: '0.3.5', 12: '0.3.5.8', 13: '0.3.6', 14: '0.3.6.8', 15: '0.3.6.9', 16: '0.3.7', 17: '0.4', 18: '0.4.5', 19: '0.4.6', 20: '0.4.7', 21: '0.5', 22: '0.5.6', 23: '0.6', 24: '1', 25: '1.2', 26: '1.2.4.6.8.10', 27: '1.2.6', 28: '1.2.6.8', 29: '1.3', 30: '1.3.5', 31: '1.3.5.8', 32: '1.3.6', 33: '1.3.7', 34: '1.3.8', 35: '1.4', 36: '1.4.6', 37: '1.4.6.9', 38: '1.4.7', 39: '1.4.7.10', 40: '1.4.7.9', 41: '1.4.8', 42: '1.5', 43: '1.5.8', 44: '1.5.9', 45: '1.6', 46: '1.7', 47: '10', 48: '10.0', 49: '10.0.2.5', 50: '10.0.3', 51: '10.0.4', 52: '10.0.5', 53: '10.1', 54: '10.1.3', 55: '10.1.3.5.6', 56: '10.1.3.6', 57: '10.1.4', 58: '10.1.4.6', 59: '10.1.5', 60: '10.11', 61: '10.11.3', 62: '10.11.3.5', 63: '10.2', 64: '10.2.3', 65: '10.2.4', 66: '10.2.5', 67: '10.3', 68: '11', 69: '11.0', 70: '11.0.4', 71: '11.0.4.6', 72: '11.0.4.7', 73: '11.0.5', 74: '11.1', 75: '11.1.4', 76: '11.1.4.5', 77: '11.1.5', 78: '11.1.6', 79: '11.2', 80: '11.2.4', 81: '11.2.4.6', 82: '11.2.4.7', 83: '11.2.5', 84: '11.2.5.7', 85: '11.2.6', 86: '11.3', 87: '11.3.5', 88: '11.3.6', 89: '11.4', 90: '11.4.5', 91: '2', 92: '2.3', 93: '2.3.7', 94: '2.3.7.10', 95: '2.3.7.9', 96: '2.4', 97: '2.4.5', 98: '2.4.5.9', 99: '2.4.6.9.11', 100: '2.4.7', 101: '2.4.7.10', 102: '2.4.8', 103: '2.4.9', 104: '2.5', 105: '2.5.7', 106: '2.5.7.10', 107: '2.5.8', 108: '2.5.8.10', 109: '2.5.8.11', 110: '2.5.9', 111: '2.6', 112: '2.6.10', 113: '2.6.7', 114: '2.6.7.9', 115: '2.6.9', 116: '2.7', 117: '2.7.8', 118: '2.8', 119: '3', 120: '3.4', 121: '3.4.8', 122: '3.4.8.10', 123: '3.5', 124: '3.5.10', 125: '3.5.7', 126: '3.5.7.10', 127: '3.5.7.8.11', 128: '3.5.8', 129: '3.5.8.11', 130: '3.6', 131: '3.6.10', 132: '3.6.7', 133: '3.6.8', 134: '3.6.9.11', 135: '3.7', 136: '3.7.10', 137: '3.7.11', 138: '3.7.9', 139: '3.7.9.10', 140: '3.8', 141: '3.8.9', 142: '3.9', 143: '4', 144: '4.10', 145: '4.5', 146: '4.5.7.11', 147: '4.5.9', 148: '4.5.9.0', 149: '4.5.9.11', 150: '4.6', 151: '4.6.10', 152: '4.6.11', 153: '4.6.7', 154: '4.6.8.11.1', 155: '4.6.9', 156: '4.6.9.11', 157: '4.7', 158: '4.7.10', 159: '4.7.11', 160: '4.7.9', 161: '4.7.9.0', 162: '4.7.9.11', 163: '4.8', 164: '4.8.10', 165: '4.8.11', 166: '4.8.9', 167: '4.8.9.11', 168: '4.9', 169: '4.9.10', 170: '5', 171: '5.10', 172: '5.10.11', 173: '5.11', 174: '5.6', 175: '5.7', 176: '5.7.0', 177: '5.7.10', 178: '5.7.8.0', 179: '5.8', 180: '5.8.0', 181: '5.8.10', 182: '5.8.11', 183: '5.9', 184: '5.9.0', 185: '5.9.11', 186: '5.9.11.0', 187: '6', 188: '6.10', 189: '6.10.0', 190: '6.10.1', 191: '6.10.11', 192: '6.10.11.1', 193: '6.11', 194: '6.11.0', 195: '6.7', 196: '6.7.0', 197: '6.7.11', 198: '6.7.11.2', 199: '6.8', 200: '6.8.0', 201: '6.8.1', 202: '6.8.10', 203: '6.8.10.0.3', 204: '6.8.11', 205: '6.8.11.2', 206: '6.9', 207: '6.9.0', 208: '6.9.0.2', 209: '6.9.1', 210: '6.9.11', 211: '6.9.11.2', 212: '7', 213: '7.0', 214: '7.10', 215: '7.10.0', 216: '7.10.0.3', 217: '7.10.1', 218: '7.10.1.3', 219: '7.10.2', 220: '7.11', 221: '7.11.0', 222: '7.11.2', 223: '7.8', 224: '7.8.0', 225: '7.8.0.3', 226: '7.8.10.2', 227: '7.8.11', 228: '7.9', 229: '7.9.0', 230: '7.9.1', 231: '7.9.11', 232: '7.9.11.2', 233: '7.9.2', 234: '8', 235: '8.0', 236: '8.0.1', 237: '8.0.2', 238: '8.0.3', 239: '8.1', 240: '8.10', 241: '8.10.0', 242: '8.10.1', 243: '8.10.11.1.4', 244: '8.10.11.3', 245: '8.10.2', 246: '8.10.3', 247: '8.11', 248: '8.11.1', 249: '8.11.1.4', 250: '8.11.2', 251: '8.11.3', 252: '8.9', 253: '8.9.1', 254: '8.9.1.3', 255: '8.9.1.4', 256: '8.9.11.1', 257: '8.9.2', 258: '9', 259: '9.0', 260: '9.0.2', 261: '9.0.2.4', 262: '9.0.2.5', 263: '9.0.3', 264: '9.0.3.5', 265: '9.0.4', 266: '9.1', 267: '9.1.4', 268: '9.10', 269: '9.10.2', 270: '9.10.2.4', 271: '9.11', 272: '9.11.0.4', 273: '9.11.1', 274: '9.11.2', 275: '9.11.2.4', 276: '9.11.2.5', 277: '9.11.3', 278: '9.11.4', 279: '9.2', 280: '9.2.3', 281: 'A0', 282: 'A1', 283: 'A2', 284: 'A3', 285: 'A4', 286: 'A5', 287: 'A6', 288: 'B-1', 289: 'B-2', 290: 'B-3', 291: 'B-4', 292: 'B-5', 293: 'B-6', 294: 'B0', 295: 'B1', 296: 'B2', 297: 'B3', 298: 'B4', 299: 'B5', 300: 'B6', 301: 'C#1', 302: 'C#2', 303: 'C#3', 304: 'C#4', 305: 'C#5', 306: 'C#6', 307: 'C#7', 308: 'C1', 309: 'C2', 310: 'C3', 311: 'C4', 312: 'C5', 313: 'C6', 314: 'C7', 315: 'D1', 316: 'D2', 317: 'D3', 318: 'D4', 319: 'D5', 320: 'D6', 321: 'D7', 322: 'E-1', 323: 'E-2', 324: 'E-3', 325: 'E-4', 326: 'E-5', 327: 'E-6', 328: 'E1', 329: 'E2', 330: 'E3', 331: 'E4', 332: 'E5', 333: 'E6', 334: 'F#1', 335: 'F#2', 336: 'F#3', 337: 'F#4', 338: 'F#5', 339: 'F#6', 340: 'F1', 341: 'F2', 342: 'F3', 343: 'F4', 344: 'F5', 345: 'F6', 346: 'G#1', 347: 'G#2', 348: 'G#3', 349: 'G#4', 350: 'G#5', 351: 'G#6', 352: 'G1', 353: 'G2', 354: 'G3', 355: 'G4', 356: 'G5', 357: 'G6', 358: 'SC'}\n",
            "[array([218.57965], dtype=float32), array([439.2564], dtype=float32), array([429.76135], dtype=float32), array([303.56592], dtype=float32), array([369.19873], dtype=float32), array([477.25565], dtype=float32), array([481.66016], dtype=float32), array([480.71094], dtype=float32), array([227.63068], dtype=float32), array([103.96164], dtype=float32), array([280.5677], dtype=float32), array([391.59473], dtype=float32), array([79.72809], dtype=float32), array([467.4469], dtype=float32), array([458.29633], dtype=float32), array([464.3813], dtype=float32), array([403.10858], dtype=float32), array([448.63257], dtype=float32), array([433.8742], dtype=float32), array([399.33606], dtype=float32), array([457.15237], dtype=float32), array([473.14642], dtype=float32), array([185.26479], dtype=float32), array([352.08594], dtype=float32), array([405.51968], dtype=float32), array([438.9154], dtype=float32), array([60.36917], dtype=float32), array([481.00366], dtype=float32), array([305.59003], dtype=float32), array([156.0911], dtype=float32), array([457.88074], dtype=float32), array([482.8592], dtype=float32), array([404.2433], dtype=float32), array([257.70206], dtype=float32), array([373.53934], dtype=float32), array([380.31152], dtype=float32), array([267.56946], dtype=float32), array([198.51926], dtype=float32), array([210.06216], dtype=float32), array([470.29388], dtype=float32), array([448.29077], dtype=float32), array([472.53033], dtype=float32), array([407.273], dtype=float32), array([471.882], dtype=float32), array([43.979614], dtype=float32), array([417.10544], dtype=float32), array([467.47568], dtype=float32), array([416.49493], dtype=float32), array([456.04874], dtype=float32), array([475.52603], dtype=float32), array([432.84378], dtype=float32), array([365.0356], dtype=float32), array([403.15244], dtype=float32), array([455.3563], dtype=float32), array([464.0047], dtype=float32), array([229.13777], dtype=float32), array([409.07254], dtype=float32), array([5.8460846], dtype=float32), array([424.63068], dtype=float32), array([366.17627], dtype=float32), array([457.75433], dtype=float32), array([325.5927], dtype=float32), array([351.03732], dtype=float32), array([456.75836], dtype=float32), array([401.7742], dtype=float32), array([471.36407], dtype=float32), array([481.43344], dtype=float32), array([18.487534], dtype=float32), array([428.086], dtype=float32), array([37.2807], dtype=float32), array([420.17017], dtype=float32), array([266.13544], dtype=float32), array([345.9353], dtype=float32), array([126.5642], dtype=float32), array([362.6556], dtype=float32), array([349.37515], dtype=float32), array([88.93753], dtype=float32), array([479.2058], dtype=float32), array([279.40594], dtype=float32), array([401.50708], dtype=float32), array([468.4233], dtype=float32), array([477.66367], dtype=float32), array([402.10425], dtype=float32), array([440.1388], dtype=float32), array([467.72833], dtype=float32), array([406.69257], dtype=float32), array([421.99255], dtype=float32), array([246.51979], dtype=float32), array([252.48203], dtype=float32), array([277.01587], dtype=float32), array([315.47223], dtype=float32), array([411.1946], dtype=float32), array([480.66815], dtype=float32), array([437.07892], dtype=float32), array([34.44522], dtype=float32), array([473.61923], dtype=float32), array([121.95461], dtype=float32), array([480.86935], dtype=float32), array([78.02544], dtype=float32), array([428.09213], dtype=float32)]\n",
            "['C#3', 'G1', 'C#5', 'E-4', 'G#6', 'F6', 'G#4', 'D1']\n",
            "tone span =39\n",
            "unique tones = 36\n",
            "['7.10.1.3', '0', '0', 'C#3', '0', '0', '0', '0', '7.8.11', '2.4.9', '9.2.3', '0', '11.2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '5.9.11', 'G1', '0', '0', '10.11', '0', 'C#5', '4.6.9.11', '0', '0', '0', '8.9.2', '0', '0', '9.1.4', '6.7.11.2', '6.9.11', '0', '0', '0', '0', '0', '1.5.8', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '7.9.0', '0', '0.2.3.7', '0', '0', '0', 'E-4', 'G#6', '0', '0', '0', '0', '0.4.5', '0', '1.4.6.9', '0', '9.1', 'F6', '3.5.7.10', '0', 'G#4', '11.3.6', '0', '9.2', '0', '0', '0', '0', '0', '0', '0', '0', '8.10.3', '8.9', '9.11.3', 'D1', '0', '0', '0', '1.3.8', '0', '3.4.8', '0', '11.1.6', '0']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}